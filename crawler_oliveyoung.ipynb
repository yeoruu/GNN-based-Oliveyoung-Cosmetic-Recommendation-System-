{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/스킨-토너.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 212\u001b[0m\n\u001b[1;32m    209\u001b[0m END_IDX \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m190\u001b[39m\n\u001b[1;32m    211\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m cat_idx, to_collect \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(category_names):\n\u001b[0;32m--> 212\u001b[0m     sub_category_href_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mto_collect\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    213\u001b[0m     sub_category_href_df_start \u001b[38;5;241m=\u001b[39m sub_category_href_df\u001b[38;5;241m.\u001b[39miloc[START_IDX:END_IDX, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    214\u001b[0m     sub_category_href_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(sub_category_href_df_start)\n",
      "File \u001b[0;32m~/anaconda3/envs/rec/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/rec/lib/python3.10/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/anaconda3/envs/rec/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/rec/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/anaconda3/envs/rec/lib/python3.10/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/스킨-토너.csv'"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "# from tqdm import tqdm_notebook\n",
    "# from collections import OrderedDict\n",
    "import time\n",
    "# import requests\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "def create_directory(directory):\n",
    "    try:\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "    except OSError:\n",
    "        print(\"Error: Failed to create the directory.\")\n",
    "\n",
    "# Selenium Driver에서 Beautiful Soup을 사용해 HTML 가져오기\n",
    "def get_soup(browser):\n",
    "    html = browser.page_source\n",
    "    return BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "\n",
    "# 페이지의 전체상품 상세링크 가져오기\n",
    "def get_product_links(soup):\n",
    "    # class가 \"prd_info\"인 모든 div 태그를 찾기\n",
    "    prd_info_divs = soup.find_all('div', class_='prd_info')\n",
    "\n",
    "    # 결과를 담을 리스트 초기화\n",
    "    href_list = []\n",
    "\n",
    "    # 각 div에서 class가 \"prd_thumb goodsList\"인 모든 a 태그의 href 속성 추출\n",
    "    for div in prd_info_divs:\n",
    "        prd_thumb_a = div.find('a', class_='prd_thumb goodsList')\n",
    "        if prd_thumb_a:\n",
    "            href = prd_thumb_a.get('href')\n",
    "            href_list.append(href)\n",
    "\n",
    "    return href_list\n",
    "\n",
    "\n",
    "# 페이지별 상품 리뷰 크롤링 함수2\n",
    "def review_crawling2(browser, category):\n",
    "    reviews = list()\n",
    "\n",
    "    # max 페이지\n",
    "    page_box = browser.find_elements(By.CSS_SELECTOR,'#gdasContentsArea > div > div.pageing > a')\n",
    "    max_page = len(page_box)\n",
    "\n",
    "    for page in range(1, max_page):\n",
    "        if page > 1:\n",
    "            try:\n",
    "                browser.find_elements(By.CSS_SELECTOR, '#gdasContentsArea > div > div.pageing > a')[page-2].click()\n",
    "                time.sleep(2)\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "        for i in range(1, 11):\n",
    "            soup = get_soup(browser)\n",
    "            selectors = {\n",
    "                \"brand\": f'#Contents > div.prd_detail_box.renew > div.right_area > div > p.prd_name',\n",
    "                \"nickname\": f'#gdasList > li:nth-child({i}) > div.info > div > p.info_user > a.id',\n",
    "                \"rate\": f'#gdasList > li:nth-child({i}) > div.review_cont > div.score_area > span.review_point > span',\n",
    "                \"skin_type\": f'#gdasList > li:nth-child({i}) > div.review_cont > div.poll_sample > dl:nth-child(1) > dd > span',\n",
    "                \"select_1_content\": f'#gdasList > li:nth-child({i}) > div.review_cont > div.poll_sample > dl:nth-child(1) > dd > span',\n",
    "                \"select_2_content\": f'#gdasList > li:nth-child({i}) > div.review_cont > div.poll_sample > dl:nth-child(2) > dd > span',\n",
    "                \"select_3_content\": f'#gdasList > li:nth-child({i}) > div.review_cont > div.poll_sample > dl:nth-child(3) > dd > span',\n",
    "                \"txt\": f'#gdasList > li:nth-child({i}) > div.review_cont > div.txt_inner'\n",
    "            }\n",
    "            # skin_type이랑 select_1_content랑 동일\n",
    "\n",
    "            contents = dict()\n",
    "\n",
    "            for name, selector in selectors.items():\n",
    "                target_element = soup.select_one(selector)\n",
    "\n",
    "                # 텍스트 가져오기\n",
    "                if target_element:\n",
    "                    text_content = target_element.get_text(strip=True)\n",
    "                    contents[name] = text_content\n",
    "                    # print(\"텍스트:\", text_content)\n",
    "                else:\n",
    "                    print(f\"{name=}-해당하는 요소를 찾을 수 없습니다.\")\n",
    "                    break\n",
    "\n",
    "            # 제품명 특수문자 제거\n",
    "            brand_string = contents[\"brand\"]\n",
    "            brand = re.sub(r'\\[.*?\\]|\\(.*?\\)', '', brand_string).strip().replace(\"  \", \" \")\n",
    "            contents[\"brand\"] = brand\n",
    "            contents[\"category\"] = category\n",
    "            try:\n",
    "                contents[\"rate\"] = contents[\"rate\"][-2]\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            reviews.append(contents)\n",
    "\n",
    "    return browser, reviews\n",
    "\n",
    "\n",
    "# 페이지별 상품 리뷰 크롤링 함수\n",
    "def review_crawling(df, current_page):\n",
    "    for i in range(1, 11):  # 한 페이지 내 10개 리뷰 크롤링\n",
    "        try:\n",
    "            id = driver.find_element(By.CSS_SELECTOR,\n",
    "                                     f'#gdasList > li:nth-child({i}) > div.info > div > p.info_user > a.id').text\n",
    "            category = '스킨/토너'\n",
    "            brand_string = driver.find_element(By.CSS_SELECTOR,\n",
    "                                               '#Contents > div.prd_detail_box.renew > div.right_area > div > p.prd_name').text\n",
    "            brand = re.sub(r'\\[.*?\\]|\\(.*?\\)', '', brand_string).strip().replace(\"  \", \" \")\n",
    "            # if brand_string[0] == '[':\n",
    "            #     brand_match = re.search(r'\\]\\s*(.+?)\\s*\\(', brand_string)\n",
    "            #     brand = brand_match.group(1)\n",
    "            #     if brand_string[]\n",
    "            # else:\n",
    "            #     brand_match = re.search(r'(.+?)\\s*\\(', brand_string)\n",
    "            #     brand = brand_match.group(0)\n",
    "            # print(\"매칭되는 부분이 없습니다.\")\n",
    "            rate = driver.find_element(By.CSS_SELECTOR,\n",
    "                                       f'#gdasList > li:nth-child({i}) > div.review_cont > div.score_area > span.review_point > span').text\n",
    "            skin_type = driver.find_element(By.CSS_SELECTOR,\n",
    "                                            f'#gdasList > li:nth-child({i}) > div.review_cont > div.poll_sample > dl:nth-child(1) > dd > span').text\n",
    "            select_1_content = driver.find_element(By.CSS_SELECTOR,\n",
    "                                                   f'#gdasList > li:nth-child({i}) > div.review_cont > div.poll_sample > dl:nth-child(1) > dd > span').text\n",
    "            select_2_content = driver.find_element(By.CSS_SELECTOR,\n",
    "                                                   f'#gdasList > li:nth-child({i}) > div.review_cont > div.poll_sample > dl:nth-child(2) > dd > span').text\n",
    "            select_3_content = driver.find_element(By.CSS_SELECTOR,\n",
    "                                                   f'#gdasList > li:nth-child({i}) > div.review_cont > div.poll_sample > dl:nth-child(3) > dd > span').text\n",
    "            txt = driver.find_element(By.CSS_SELECTOR,\n",
    "                                      f'#gdasList > li:nth-child({i}) > div.review_cont > div.txt_inner').text\n",
    "            df.loc[len(df)] = [id, category, brand, rate, skin_type, select_1_content, select_2_content,\n",
    "                               select_3_content, txt]\n",
    "            time.sleep(3)\n",
    "        except Exception as e:\n",
    "            print(f\"에러 발생: {str(e)}\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def save_all_products_in_sub_category(name, path):\n",
    "    # 웹 드라이버 설정\n",
    "    driver = webdriver.Chrome()\n",
    "    driver.get(path)\n",
    "\n",
    "    # 48개씩 보기 클릭\n",
    "    show48 = \"#Contents > div.cate_align_box > div.count_sort.tx_num > ul > li:nth-child(3) > a\"\n",
    "    driver.find_element(By.CSS_SELECTOR, show48).click()\n",
    "\n",
    "    # 소분류 전체 상품 링크\n",
    "    # max 페이지\n",
    "    page_box = driver.find_elements(By.CSS_SELECTOR, '#Container > div.pageing > a')\n",
    "    max_page = len(page_box)\n",
    "\n",
    "    sub_category_href_list = list()\n",
    "\n",
    "    for i in range(1, max_page + 1):\n",
    "        soup = get_soup(driver)\n",
    "        href_list = get_product_links(soup)\n",
    "        sub_category_href_list.extend(href_list)\n",
    "\n",
    "        # 페이지 이동\n",
    "        if i == max_page:\n",
    "            break\n",
    "\n",
    "        a_index = (i - 1) % 10 + 1\n",
    "        a_element_xpath = f'//*[@id=\"Container\"]/div[2]/a[{a_index}]'\n",
    "        driver.find_element(By.XPATH, a_element_xpath).click()\n",
    "\n",
    "    # csv 저장\n",
    "    df = pd.DataFrame(sub_category_href_list, columns=['href'])\n",
    "    file_name = f\"data/{name}.csv\"\n",
    "    df.to_csv(file_name, index=False)\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "###########################################################################\n",
    "'''0. 폴더 생성'''\n",
    "category_directories = [\"./data/skin\", \"./data/essence\", \"./data/cream\", \"./data/lotion\", \"./data/oil\"]\n",
    "for name in category_directories:\n",
    "    create_directory(name)\n",
    "\n",
    "''' 1. 카테고리 별 제품 링크 수집 (카테고리당 최대 480개 씩)\n",
    "(수집할 링크 목록 모을 때 사용)\n",
    "'''\n",
    "# category_dict = {\n",
    "#     \"스킨-토너\": \"https://www.oliveyoung.co.kr/store/display/getMCategoryList.do?dispCatNo=100000100010013&isLoginCnt=0&aShowCnt=0&bShowCnt=0&cShowCnt=0&trackingCd=Cat100000100010013_MID&trackingCd=Cat100000100010013_MID&t_page=%EC%B9%B4%ED%85%8C%EA%B3%A0%EB%A6%AC%EA%B4%80&t_click=%EC%B9%B4%ED%85%8C%EA%B3%A0%EB%A6%AC%EC%83%81%EC%84%B8_%EC%A4%91%EC%B9%B4%ED%85%8C%EA%B3%A0%EB%A6%AC&t_2nd_category_type=%EC%A4%91_%EC%8A%A4%ED%82%A8%2F%ED%86%A0%EB%84%88\",\n",
    "#     \"에센스-세럼-앰플\": \"https://www.oliveyoung.co.kr/store/display/getMCategoryList.do?dispCatNo=100000100010014&isLoginCnt=0&aShowCnt=0&bShowCnt=0&cShowCnt=0\",\n",
    "#     \"크림\": \"https://www.oliveyoung.co.kr/store/display/getMCategoryList.do?dispCatNo=100000100010015&isLoginCnt=0&aShowCnt=0&bShowCnt=0&cShowCnt=0\",\n",
    "#     \"로션\": \"https://www.oliveyoung.co.kr/store/display/getMCategoryList.do?dispCatNo=100000100010016&isLoginCnt=0&aShowCnt=0&bShowCnt=0&cShowCnt=0\",\n",
    "#     \"미스트-오일\": \"https://www.oliveyoung.co.kr/store/display/getMCategoryList.do?dispCatNo=100000100010010&isLoginCnt=0&aShowCnt=0&bShowCnt=0&cShowCnt=0\"\n",
    "# }\n",
    "#\n",
    "# for name, path in category_dict.items():\n",
    "#     save_all_products_in_sub_category(name, path)\n",
    "\n",
    "'''2. 카테고리 제품 정보, 리뷰 데이터 수집'''\n",
    "\n",
    "# category_names = [\"스킨-토너\", \"에센스-세럼-앰플\", \"크림\", \"로션\", \"미스트-오일\"]\n",
    "# category_path = [\"./data/skin\", \"./data/essence\", \"./data/cream\", \"./data/lotion\", \"./data/oil\"]\n",
    "category_names = [\"스킨-토너\", \"에센스-세럼-앰플\", \"크림\", \"로션\"]\n",
    "category_path = [\"./data/skin\", \"./data/essence\", \"./data/cream\", \"./data/lotion\"]\n",
    "START_IDX = 100\n",
    "END_IDX = 190\n",
    "\n",
    "for cat_idx, to_collect in enumerate(category_names):\n",
    "    sub_category_href_df = pd.read_csv(f\"data/{to_collect}.csv\")\n",
    "    sub_category_href_df_start = sub_category_href_df.iloc[START_IDX:END_IDX, -1]\n",
    "    sub_category_href_list = list(sub_category_href_df_start)\n",
    "\n",
    "    # 웹 드라이버 설정\n",
    "    driver = webdriver.Chrome()\n",
    "\n",
    "    # 데이터 셋 생성\n",
    "    review_cols = ['nickname', 'brand', 'category', 'rate', 'skin_type', 'select_1_content', 'select_2_content', 'select_3_content', 'txt']\n",
    "    df_review_cos1 = pd.DataFrame(columns=review_cols)\n",
    "    df_ingredient = pd.DataFrame(columns=['name', 'ingredient'])\n",
    "\n",
    "    # 3. 소분류 제품 수집\n",
    "    for idx, link in enumerate(sub_category_href_list):\n",
    "        idx += START_IDX\n",
    "        driver.get(link)\n",
    "\n",
    "        # 리뷰 버튼 클릭\n",
    "        review_button = driver.find_element(By.XPATH, '//*[@id=\"reviewInfo\"]/a')\n",
    "        review_button.click()\n",
    "        time.sleep(1)\n",
    "\n",
    "        # 리뷰 데이터 수집 (제품 1개)\n",
    "        driver, review_data = review_crawling2(driver, to_collect)\n",
    "        try:\n",
    "            tmp = pd.DataFrame(review_data)[review_cols]\n",
    "            df_review_cos1 = pd.concat([df_review_cos1, tmp], axis=0)\n",
    "            print(f\"{idx=}, {len(df_review_cos1)=}, {df_review_cos1.tail(1)}\")\n",
    "        except Exception as e:\n",
    "            print(\"idx: \", idx, e)\n",
    "            continue\n",
    "\n",
    "        if len(df_review_cos1) >= 1000:\n",
    "            filename = f\"{category_path[cat_idx]}/{to_collect}-{idx}.csv\"\n",
    "            df_review_cos1.to_csv(filename, index=False)\n",
    "            df_review_cos1 = pd.DataFrame(columns=review_cols)\n",
    "            print(f\"created: {filename}\")\n",
    "\n",
    "    df_review_cos1.to_csv(f\"{category_path[cat_idx]}/{to_collect}-final.csv\", index=False)\n",
    "    df_review_cos1 = pd.DataFrame(columns=review_cols)\n",
    "\n",
    "\n",
    "\n",
    "# # 전체 페이지 수 설정\n",
    "# total_pages = 15\n",
    "#\n",
    "# # 페이지 번호 초기화\n",
    "# page_number = 1\n",
    "#\n",
    "# while page_number <= 15:  # 페이지 끝 임의로 설정\n",
    "#     # 현재 페이지의 모든 화장품 이미지 버튼을 클릭\n",
    "#     for row in range(1, 7):\n",
    "#         time.sleep(2)\n",
    "#         for col in range(1, 5):\n",
    "#             image_button_xpath = f'//*[@id=\"Contents\"]/ul[{row + 1}]/li[{col}]/div/a'\n",
    "#             image_button = driver.find_element(By.XPATH, image_button_xpath)\n",
    "#             image_button.click()\n",
    "#             time.sleep(2)\n",
    "#\n",
    "#             # 리뷰 버튼 클릭\n",
    "#             review_button = driver.find_element(By.XPATH, '//*[@id=\"reviewInfo\"]/a')\n",
    "#             review_button.click()\n",
    "#             time.sleep(1)\n",
    "#\n",
    "#             # 리뷰 데이터 수집\n",
    "#             review_data = review_crawling(df_review_cos1, page_number)\n",
    "#\n",
    "#             # 리뷰 및 정보 수집\n",
    "#             # for page_number in range(1, total_pages + 1):\n",
    "#             #     review_data=review_crawling(df_review_cos1, page_number)\n",
    "#\n",
    "#             # # 데이터프레임에 데이터 추가\n",
    "#             # if review_data is not None:\n",
    "#             #     df_review_cos1 = df_review_cos1.append(review_data, ignore_index=True)\n",
    "#             #     page_number += 1\n",
    "#\n",
    "#             # 구매 정보 클릭\n",
    "#             ingredient_button = driver.find_element(By.XPATH, '//*[@id=\"buyInfo\"]')\n",
    "#             ingredient_button.click()\n",
    "#             time.sleep(2)\n",
    "#\n",
    "#             # 이름, 성분 정보 수집\n",
    "#             name_string = driver.find_element(By.CSS_SELECTOR,\n",
    "#                                               '#Contents > div.prd_detail_box.renew > div.right_area > div > p.prd_name').text\n",
    "#             name = re.sub(r'\\[.*?\\]|\\(.*?\\)', '', name_string).strip().replace(\"  \", \" \")\n",
    "#\n",
    "#             ingredient = driver.find_element(By.CSS_SELECTOR, '#artcInfo > dl:nth-child(8) > dd').text\n",
    "#\n",
    "#             # df_ingredient에 데이터 추가\n",
    "#             df_to_add = pd.DataFrame({'name': [name], 'ingredient': [ingredient]})\n",
    "#             df_ingredient = pd.concat([df_ingredient, df_to_add], ignore_index=True)\n",
    "#\n",
    "#             # df_review_cos1에 데이터 추가\n",
    "#             df_review_cos1 = pd.concat([df_review_cos1, review_data], axis=0)\n",
    "#\n",
    "#             # 이미지 목록이 있는 페이지로 돌아가기\n",
    "#             driver.back()\n",
    "#\n",
    "#     # 리뷰 csv 파일 저장\n",
    "#     if df_review_cos1 is not None:\n",
    "#         df_review_cos1.to_csv(f'./scraping/data/review_data{page_number}.csv', index=False)\n",
    "#\n",
    "#     # 페이지 이동 후 로딩을 기다리기 위한 시간 지연\n",
    "#     time.sleep(2)\n",
    "#\n",
    "#     # 10 페이지가 되면 화살표 버튼을 클릭하여 다음 페이지로 이동\n",
    "#     if page_number % 10 == 0:\n",
    "#         # arrow_button_xpath = '//*[@id=\"Container\"]/div[2]/a[10]'\n",
    "#         # driver.find_element(By.XPATH, arrow_button_xpath).click()\n",
    "#         # time.sleep(2)\n",
    "#         break\n",
    "#     else:\n",
    "#         # 10 페이지가 아니면 a 태그 클릭\n",
    "#         a_index = (page_number - 1) % 10 + 1\n",
    "#         a_element_xpath = f'//*[@id=\"Container\"]/div[2]/a[{a_index}]'\n",
    "#         driver.find_element(By.XPATH, a_element_xpath).click()\n",
    "#\n",
    "#     # 페이지 번호 증가\n",
    "#     page_number += 1\n",
    "#\n",
    "# # 성분 csv 파일 저장\n",
    "# df_ingredient.to_csv(f'./scraping/data/ingredient_data{page_number}.csv', index=False)\n",
    "#\n",
    "# # 웹 드라이버 종료\n",
    "# driver.quit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (rec)",
   "language": "python",
   "name": "rec"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
