import os
import re
import time
import pandas as pd
from playwright.sync_api import sync_playwright, TimeoutError as PWTimeoutError

INPUT_TABLE2 = "table2_essence_basic.csv"   # ì´ë¯¸ ë§Œë“  í…Œì´ë¸”2
OUT_TABLE3 = "table3_essence_ingredients.csv"

POLITE_SLEEP = 0.8          # ì„œë²„ ì˜ˆì˜(ì°¨ë‹¨ ë°©ì§€)
RETRY = 2                   # ì œí’ˆ ë‹¨ìœ„ ì¬ì‹œë„ íšŸìˆ˜
SAVE_EVERY = 20             # Nê°œë§ˆë‹¤ ì²´í¬í¬ì¸íŠ¸ ì €ì¥

TARGET_ROW_LABEL = "í™”ì¥í’ˆë²•ì— ë”°ë¼ ê¸°ì¬í•´ì•¼ í•˜ëŠ” ëª¨ë“  ì„±ë¶„"
TOGGLE_TITLE = "ìƒí’ˆì •ë³´ ì œê³µê³ ì‹œ"

def load_done_urls():
    """ì´ë¯¸ í¬ë¡¤ë§í•œ URL ëª©ë¡ ë¡œë“œ"""
    if os.path.exists(OUT_TABLE3):
        df = pd.read_csv(OUT_TABLE3)
        done = set(df["product_url"].astype(str).tolist())
        return df, done
    else:
        df = pd.DataFrame(columns=["product_id", "product_url", "ingredients", "ok", "error"])
        return df, set()

def normalize_text(s: str) -> str:
    """í…ìŠ¤íŠ¸ ì •ê·œí™” (ê³µë°± ì •ë¦¬)"""
    if s is None:
        return None
    s = re.sub(r"\s+", " ", s).strip()
    return s

def open_and_extract(page, url: str) -> str:
    """
    1) ìƒì„¸ í˜ì´ì§€ ì ‘ì†
    2) 'ìƒí’ˆì •ë³´ ì œê³µê³ ì‹œ' ì„¹ì…˜ì„ í¼ì¹¨(í† ê¸€)
    3) í‘œì—ì„œ TARGET_ROW_LABEL í–‰ì„ ì°¾ì•„ td í…ìŠ¤íŠ¸ ë°˜í™˜
    """
    page.goto(url, wait_until="domcontentloaded")
    page.wait_for_timeout(600)

    # âœ… 1) 'ìƒí’ˆì •ë³´ ì œê³µê³ ì‹œ' í† ê¸€ í¼ì¹˜ê¸°
    # - í…ìŠ¤íŠ¸ ê¸°ë°˜ìœ¼ë¡œ ì°¾ê³  í´ë¦­ (ì‚¬ì´íŠ¸ êµ¬ì¡° ë°”ë€Œì–´ë„ ì‚´ì•„ë‚¨ê²Œ)
    # - ì´ë¯¸ í¼ì³ì ¸ ìˆìœ¼ë©´ í´ë¦­í•´ë„ í° ë¬¸ì œ ì—†ê²Œ ì„¤ê³„
    toggle = page.locator(f"text={TOGGLE_TITLE}").first
    if toggle.count() > 0:
        # í† ê¸€ í´ë¦­ ê°€ëŠ¥í•˜ë„ë¡ ë³´ì´ê²Œ ì´ë™
        try:
            toggle.scroll_into_view_if_needed(timeout=2000)
        except:
            page.evaluate("window.scrollTo(0, document.body.scrollHeight * 0.65);")
            page.wait_for_timeout(300)

        # í´ë¦­ ì‹œë„ (ê°€ë” í…ìŠ¤íŠ¸ ìì²´ê°€ ì•„ë‹Œ ìƒìœ„ í—¤ë”ê°€ í´ë¦­ ëŒ€ìƒì¼ ìˆ˜ ìˆì–´ ancestorë„ ì‹œë„)
        clicked = False
        try:
            toggle.click(timeout=1500)
            clicked = True
        except:
            try:
                toggle.locator("xpath=ancestor::*[1]").click(timeout=1500)
                clicked = True
            except:
                pass

        if clicked:
            page.wait_for_timeout(500)

    # âœ… 2) í‘œì—ì„œ "í™”ì¥í’ˆë²•ì— ë”°ë¼ ê¸°ì¬..." í–‰ ì°¾ê¸°
    # ì „ëµ:
    #   - thì— TARGET_ROW_LABEL í¬í•¨í•˜ëŠ” trì„ ì°¾ê³ 
    #   - ê·¸ trì˜ td í…ìŠ¤íŠ¸ë¥¼ ì½ëŠ”ë‹¤.
    row = page.locator(f"xpath=//tr[.//th[contains(normalize-space(.), '{TARGET_ROW_LABEL}')]]").first
    if row.count() == 0:
        # fallback: í˜ì´ì§€ ì „ì²´ í…ìŠ¤íŠ¸ì—ì„œ ë¼ë²¨ì´ ë³´ì´ëŠ”ì§€ í™•ì¸ í›„ ì¢€ ë” ë„“ê²Œ íƒìƒ‰
        # (í…Œì´ë¸”ì´ div êµ¬ì¡°ë¡œ ë°”ë€ŒëŠ” ê²½ìš° ëŒ€ë¹„)
        key = page.locator(f"text={TARGET_ROW_LABEL}").first
        if key.count() == 0:
            raise RuntimeError("TARGET_LABEL_NOT_FOUND")

        # key ì£¼ë³€ì—ì„œ ê°€ì¥ ê°€ê¹Œìš´ td/ë‚´ìš© í›„ë³´ íƒìƒ‰(ìµœí›„)
        # - ê°™ì€ í–‰ì˜ ë‹¤ìŒ í˜•ì œ ìš”ì†Œ í…ìŠ¤íŠ¸ ë“±ì„ ì‹œë„
        try:
            key.scroll_into_view_if_needed(timeout=1500)
        except:
            pass

        # ê·¼ì²˜ì˜ í‘œ ì…€ í›„ë³´(td)ë“¤ ì¤‘ ê°€ì¥ ê¸´ í…ìŠ¤íŠ¸ë¥¼ ì„±ë¶„ìœ¼ë¡œ ê°€ì •
        tds = page.locator("td")
        best = ""
        for i in range(min(tds.count(), 80)):
            txt = normalize_text(tds.nth(i).inner_text())
            if txt and len(txt) > len(best) and ("," in txt or "ì •ì œìˆ˜" in txt or "ê¸€ë¦¬ì„¸" in txt):
                best = txt
        if not best:
            raise RuntimeError("INGREDIENTS_NOT_FOUND_FALLBACK")
        return best

    # ì •ìƒ ì¼€ì´ìŠ¤: í•´ë‹¹ trì˜ td ê°€ì ¸ì˜¤ê¸°
    cell = row.locator("td").first
    if cell.count() == 0:
        raise RuntimeError("TD_NOT_FOUND_IN_ROW")

    ingredients = normalize_text(cell.inner_text())
    if not ingredients:
        raise RuntimeError("EMPTY_INGREDIENTS")

    return ingredients

def scrape_ingredients(headless=True, verbose=True):
    """ëª¨ë“  ì œí’ˆì˜ ì„±ë¶„ ì •ë³´ í¬ë¡¤ë§"""
    if verbose:
        print("="*60)
        print("ğŸ§ª ì˜¬ë¦¬ë¸Œì˜ ì„±ë¶„ í¬ë¡¤ëŸ¬ ì‹œì‘")
        print("="*60)
        print(f"ì…ë ¥ íŒŒì¼: {INPUT_TABLE2}")
        print(f"ì¶œë ¥ íŒŒì¼: {OUT_TABLE3}")
        print("="*60)
    
    # í…Œì´ë¸”2 íŒŒì¼ ë¡œë“œ
    if not os.path.exists(INPUT_TABLE2):
        print(f"âŒ ì˜¤ë¥˜: {INPUT_TABLE2} íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return None
    
    t2 = pd.read_csv(INPUT_TABLE2)
    # í•„ìš”í•œ ì»¬ëŸ¼ë§Œ
    t2 = t2[["product_id", "product_url"]].dropna().drop_duplicates(subset=["product_url"]).reset_index(drop=True)

    out_df, done_urls = load_done_urls()
    total = len(t2)
    
    if verbose:
        print(f"\nğŸ“Š ì´ ì œí’ˆ ìˆ˜: {total}ê°œ")
        print(f"ğŸ“Š ì´ë¯¸ ì™„ë£Œëœ ì œí’ˆ: {len(done_urls)}ê°œ")
        print(f"ğŸ“Š ë‚¨ì€ ì œí’ˆ: {total - len(done_urls)}ê°œ")
        print("="*60)

    with sync_playwright() as p:
        browser = p.chromium.launch(headless=headless)
        context = browser.new_context(
            locale="ko-KR",
            user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120 Safari/537.36",
            viewport={"width": 1280, "height": 900}
        )
        page = context.new_page()

        processed = 0
        success_count = 0
        fail_count = 0
        
        for idx, r in t2.iterrows():
            pid = str(r["product_id"])
            url = str(r["product_url"])

            if url in done_urls:
                if verbose:
                    print(f"[{idx+1}/{total}] â­ï¸  ê±´ë„ˆëœ€ (ì´ë¯¸ ì™„ë£Œ): {url}")
                continue

            ok = False
            err = ""
            ingredients = None

            if verbose:
                print(f"\n[{idx+1}/{total}] ğŸ” ì²˜ë¦¬ ì¤‘: {url}")

            for attempt in range(RETRY + 1):
                try:
                    ingredients = open_and_extract(page, url)
                    ok = True
                    err = ""
                    success_count += 1
                    if verbose:
                        print(f"  âœ… ì„±ê³µ! (ì‹œë„ {attempt+1}/{RETRY+1})")
                        if ingredients:
                            print(f"  ğŸ“‹ ì„±ë¶„ ê¸¸ì´: {len(ingredients)}ì")
                    break
                except Exception as e:
                    err = f"{type(e).__name__}:{str(e)}"
                    if verbose and attempt < RETRY:
                        print(f"  âš ï¸  ì‹¤íŒ¨ (ì‹œë„ {attempt+1}/{RETRY+1}): {err}")
                    # ê°€ë” íŒì—…/ë¡œë”© ê¼¬ì„ ë°©ì§€ìš© ë¦¬ë¡œë“œ
                    try:
                        page.wait_for_timeout(400)
                    except:
                        pass
                    if attempt < RETRY:
                        page.wait_for_timeout(800)
            
            if not ok:
                fail_count += 1
                if verbose:
                    print(f"  âŒ ìµœì¢… ì‹¤íŒ¨: {err}")

            out_df = pd.concat([out_df, pd.DataFrame([{
                "product_id": pid,
                "product_url": url,
                "ingredients": ingredients,
                "ok": ok,
                "error": err
            }])], ignore_index=True)

            done_urls.add(url)
            processed += 1

            # ì²´í¬í¬ì¸íŠ¸ ì €ì¥
            if processed % SAVE_EVERY == 0:
                out_df.to_csv(OUT_TABLE3, index=False, encoding="utf-8-sig")
                if verbose:
                    print(f"\nğŸ’¾ [ì²´í¬í¬ì¸íŠ¸ ì €ì¥] {processed}ê°œ ì²˜ë¦¬ ì™„ë£Œ -> {OUT_TABLE3}")
                    print(f"   ì„±ê³µ: {success_count}ê°œ, ì‹¤íŒ¨: {fail_count}ê°œ")

            if not verbose:
                print(f"[{len(done_urls)}/{total}] ok={ok} url={url} err={err[:80]}")
            
            time.sleep(POLITE_SLEEP)

        # final save
        out_df.to_csv(OUT_TABLE3, index=False, encoding="utf-8-sig")
        browser.close()

    if verbose:
        print("\n" + "="*60)
        print("âœ… í¬ë¡¤ë§ ì™„ë£Œ!")
        print("="*60)
        print(f"ğŸ“Š ì´ ì²˜ë¦¬: {processed}ê°œ")
        print(f"âœ… ì„±ê³µ: {success_count}ê°œ")
        print(f"âŒ ì‹¤íŒ¨: {fail_count}ê°œ")
        print(f"ğŸ’¾ ì €ì¥ íŒŒì¼: {OUT_TABLE3}")
        print("="*60)
    else:
        print(f"âœ… Done. Saved: {OUT_TABLE3}")
    
    return out_df

if __name__ == "__main__":
    import sys
    
    # ëª…ë ¹ì¤„ ì¸ìë¡œ headless ëª¨ë“œ ì œì–´ (ê¸°ë³¸ê°’: False - ë¸Œë¼ìš°ì € ë³´ì´ê¸°)
    headless_mode = "--headless" in sys.argv if len(sys.argv) > 1 else False
    verbose_mode = "--quiet" not in sys.argv  # --quietê°€ ì—†ìœ¼ë©´ verbose ëª¨ë“œ
    
    print("\nğŸ’¡ íŒ: ë¸Œë¼ìš°ì €ë¥¼ ë³´ë ¤ë©´ headless=Falseë¡œ ì„¤ì •í•˜ì„¸ìš”")
    print("   ì„±ë¶„ ì¶”ì¶œ ê³¼ì •ì„ ì‹¤ì‹œê°„ìœ¼ë¡œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!\n")
    
    df3 = scrape_ingredients(headless=headless_mode, verbose=verbose_mode)
    
    if df3 is not None and len(df3) > 0:
        print("\n" + "="*60)
        print("ğŸ“‹ ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸°")
        print("="*60)
        # ì„±ê³µí•œ í•­ëª©ë§Œ ë¯¸ë¦¬ë³´ê¸°
        success_df = df3[df3["ok"] == True]
        if len(success_df) > 0:
            print(success_df[["product_id", "ok", "ingredients"]].head(10))
            print(f"\nğŸ“Š ì„±ë¶„ ì¶”ì¶œ ì„±ê³µ: {len(success_df)}ê°œ")
        else:
            print("âš ï¸  ì„±ê³µí•œ í•­ëª©ì´ ì—†ìŠµë‹ˆë‹¤.")
        
        # ì‹¤íŒ¨í•œ í•­ëª© í™•ì¸
        fail_df = df3[df3["ok"] == False]
        if len(fail_df) > 0:
            print(f"\nâŒ ì„±ë¶„ ì¶”ì¶œ ì‹¤íŒ¨: {len(fail_df)}ê°œ")
            print("ì‹¤íŒ¨ ì›ì¸:")
            print(fail_df["error"].value_counts())
        print("="*60)

